{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4475382b",
   "metadata": {},
   "source": [
    "## 3-5. 앙상블 모델\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0a58be",
   "metadata": {},
   "source": [
    "### 1. 앙상블(Ensemble)\n",
    "\n",
    "- 여러 개의 예측 모델을 조합하여 우수한 일반화 성능을 얻는 법\n",
    "\n",
    "> $𝐹_𝑀(𝐱)=\\sum_{𝑚=1}^𝑀\\gamma_𝑚𝑓_𝑚(𝐱)$\n",
    "> \n",
    "> M: 모델 개수\n",
    "> \n",
    "> $𝑓_1(𝐱),𝑓_2(𝐱), …,𝑓_𝑀(𝐱)$: 기반 모델\n",
    "> \n",
    "> $\\gamma_1,\\gamma_2, …,\\gamma_𝑀$: 가중치\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292493bc",
   "metadata": {},
   "source": [
    "#### 앙상블의 원리\n",
    "- 여러 개의 다양한 예측 모델을 결합하면, 단일 모델보다 일반화 성능 향상 \n",
    "- 앙상블에서 효과를 얻기 위한 요건\n",
    "> 개별 모델들의 결과는 독립적\n",
    "> \n",
    "> 개별 모델들은 다양성을 지님\n",
    "> \n",
    "> 개별 모델들이 완전히 틀리지만은 않아야 함 (기여도를 지님)\n",
    "> \n",
    "> 효과적인 결합 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1053eda0",
   "metadata": {},
   "source": [
    "### 2. 배깅(Bagging)\n",
    "\n",
    "#### 배깅(Bagging): Bootstrap + Aggregation\n",
    "\n",
    "- 부트스트랩(Bootstrap)\n",
    "> 전체 데이터에서 복원 추출(with replacement)을 통해 무작위로 데이터셋을 생성하는 방법\n",
    "> \n",
    "> 전체 데이터만큼 복원 추출 시, 추출이 되지 않을 확률\n",
    "\n",
    "$= \\left(1 - \\frac{1}{n}\\right)^n \\approx e^{-1} \\approx 0.368$\n",
    "\n",
    "$\\Rightarrow$  36.8%는 Out-of-Bag(OOB) 샘플이 됨 \n",
    "\n",
    "$\\Rightarrow$ 데이터의다양성 부여, 다양한 모델 학습, 모델 검증으로 활용\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ff7933",
   "metadata": {},
   "source": [
    "#### Aggregation(집계)\n",
    "\n",
    "> $𝐹_𝑀(𝐱)=\\frac{𝟏}{𝑴}\\sum_{𝑚=1}^𝑀𝑓_𝑚(𝐱)$ $\\Rightarrow$ 분산의 감소 $\\Rightarrow$ 예측의 안정성 부여\n",
    ">\n",
    "> M: 모델 개수\n",
    "> \n",
    "> $𝑓_1(𝐱),𝑓_2(𝐱), …,𝑓_𝑀(𝐱)$: 기반 모델\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7e61bb",
   "metadata": {},
   "source": [
    "#### 랜덤 포레스트(Random Forest): 의사 결정 나무(Decision Tree)가 기반 모델인 배깅(Bagging) 모델\n",
    "\n",
    "- 의사결정나무\n",
    "\n",
    "> 데이터 변경에 따라 구조의 변경이 심함 $\\Rightarrow$ Bagging 을 통해 다양한 기반 모델 구성\n",
    "> \n",
    "> 데이터에 맞춘 가설 공간 구성 가능 $\\Rightarrow$ 편향이 낮은 강한 학습기(Strong Learner)를 만들 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a97225",
   "metadata": {},
   "source": [
    "### 3. 부스팅(Boosting)\n",
    "- 모델들을 순차적 으로 결합하여 단계적으로 오차를 줄여 보다 강력한 예측 모델을 만드는 앙상블 기법\n",
    "- Boosting 과정\n",
    "\n",
    "> $F_m(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) + \\underset{\\gamma_m, f_m}{\\text{argmin}} \\sum_{i=1}^{n} L(y_i, F_{m-1}(x_i) + \\gamma_i f_m(x_i))$\n",
    "> \n",
    ">- $L$: 손실 함수, $n$: 학습 데이터수\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73132aad",
   "metadata": {},
   "source": [
    "#### AdaBoost(Adaptive Boosting)\n",
    "- 손실이 큰 데이터 포인트일수록 높은 가중치를 부여하여 기반 모델 학습\n",
    "- 학습 알고리즘\n",
    "> 1. $\\mathbf{w}$(가중치) 초기화 $\\sum_{i=1}^nw_i=1, m=1, F_0=-0$\n",
    ">\n",
    "> 2. $\\mathbf{w}$에 따라 $f_m$학습\n",
    ">\n",
    "> 3. $\\gamma_m$의 최적값 계산\n",
    ">    \n",
    "> 4. 병합:$F_m(\\mathbf{x})=F_{m-1}(x)+\\text{learning rate} \\times \\gamma_mf_m(\\mathbf{x})$\n",
    ">\n",
    "> 5. $\\mathbf{w}$ 갱신\n",
    ">\n",
    "> 6. m=m + 1, 종료 조건 충족 때까지 2부터 반복\n",
    "\n",
    "- $\\gamma_m$의 최적값 계산\n",
    "\n",
    "> $E = \\sum_{i=1}^{n} L(y_i, F_m(\\mathbf{x}_i)) = \\sum_{i=1}^{n} L(y_i, F_{m-1}(\\mathbf{x}_i) + \\gamma_m f_m(\\mathbf{x}_i))$\n",
    ">\n",
    "> $E$가 최소인 $\\gamma_m$을 계산\n",
    "\n",
    "- $F_m(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) + learning\\ rate \\times \\gamma_m f_m(\\mathbf{x})$\n",
    "\n",
    "- $\\mathbf{w}$갱신\n",
    "\n",
    "> $\\mathbf{w} = L(y, F_m(\\mathbf{x}))$ $\\mathbf{w} = \\frac{\\mathbf{w}}{\\sum_{i=1}^{n} w_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db76902d",
   "metadata": {},
   "source": [
    "- $\\gamma_m$의 최적값 계산 예제\n",
    "\n",
    "Exponential Loss : $L(y_i, F_m(\\mathbf{x}_i)) = e^{-y_i F_m(\\mathbf{x}_i)}$, $y_i \\in \\{-1, 1\\}$\n",
    "\n",
    "$E = \\sum_{i=1}^{n} L(y_i, F_{m-1}(\\mathbf{x}_i) + \\gamma_m f_m(\\mathbf{x}_i)) = \\sum_{i=1}^{n} e^{-y_i(F_{m-1}(\\mathbf{x}_i) + \\gamma_m f_m(\\mathbf{x}_i))} = \\sum_{i=1}^{n} e^{-y_i F_{m-1}(\\mathbf{x}_i)} e^{-y_i \\gamma_m f_m(\\mathbf{x}_i)}$\n",
    "\n",
    "> $l = L(y, F_{m-1}(X))$\n",
    ">\n",
    "> $f_m(\\mathbf{x}_i)$의 예측값과 $y_i$와 일치하면, $y_i \\gamma_m f_m(\\mathbf{x}_i)$는 $\\gamma_m$ 아니면 $-\\gamma_m$\n",
    "\n",
    "$E = \\sum_{y_i = f_m(\\mathbf{x}_i)} l_i e^{-\\gamma_m} + \\sum_{y_i \\neq f_m(\\mathbf{x}_i)} l_i e^{\\gamma_m}$\n",
    "\n",
    "> $\\frac{dE}{d\\gamma_m} = 0$일 때, E가 최소\n",
    "\n",
    "$\\frac{dE}{d\\gamma_m} = 0$인 $\\gamma_m$을 구함\n",
    "\n",
    "$\\frac{dE}{dr_m} = \\sum_{y_i = f_m(\\mathbf{x}_i)} -\\gamma_m l_i e^{-\\gamma_m} + \\sum_{y_i \\neq f_m(\\mathbf{x}_i)} \\gamma_m l_i e^{\\gamma_m} = 0$\n",
    "\n",
    "$\\gamma_m = \\frac{1}{2} \\ln \\left( \\frac{\\sum_{y_i = f_m(\\mathbf{x}_i)} l_i}{\\sum_{y_i \\neq f_m(\\mathbf{x}_i)} l_i} \\right)$\n",
    "\n",
    "$\\epsilon_m = \\frac{\\sum_{y_i \\neq f_m(\\mathbf{x}_i)} l_i}{\\sum_{i=1}^{n} l_i}$\n",
    "\n",
    "$\\gamma_m = \\frac{1}{2} \\ln \\left( \\frac{1 - \\epsilon_m}{\\epsilon_m} \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b81136",
   "metadata": {},
   "source": [
    "- 학습 알고리즘: Exponential Loss: $L(y_i, F_m(\\mathbf{x}_i)) = e^{-y_i F_m(\\mathbf{x}_i)}$, $y_i \\in \\{-1, 1\\}$\n",
    "> 1. $\\mathbf{w}$(가중치) 초기화 $\\sum_{i=1}^nw_i=1, m=1, F_0=-0$\n",
    ">\n",
    "> 2. $\\mathbf{w}$에 따라 $f_m$학습\n",
    ">\n",
    "> 3. $\\gamma_m$의 최적값 계산\n",
    ">\n",
    ">    $l = L(y, F_{m-1}(X))$, $\\epsilon_m = \\frac{\\sum_{y_i \\neq f_m(\\mathbf{x}_i)} l_i}{\\sum_{i=1}^{n} l_i}$, $\\gamma_m = \\frac{1}{2} \\ln \\left( \\frac{1 - \\epsilon_m}{\\epsilon_m} \\right)$\n",
    "> \n",
    "> 4. 병합: $F_m(\\mathbf{x})=F_{m-1}(x)+\\text{learning rate} \\times \\gamma_mf_m(\\mathbf{x})$\n",
    ">\n",
    "> 5. $\\mathbf{w}$ 갱신\n",
    ">\n",
    ">    $\\mathbf{w} = L(y, F_m(\\mathbf{x}))$ $\\mathbf{w} = \\frac{\\mathbf{w}}{\\sum_{i=1}^{n} w_i}$\n",
    ">\n",
    "> 7. m=m + 1, 종료 조건 충족 때까지 2부터 반복"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4688387a",
   "metadata": {},
   "source": [
    "#### 경사 부스팅(Gradient Boosting)\n",
    "- 손실의 경사도를 맞추는 기반 모델 추가\n",
    "- 학습 알고리즘\n",
    "\n",
    "> 1. $F_0=\\underset{\\boldsymbol{const}}{min} L(y_i, const)$ 손실을 최소화하는 절편 산출, m=1\n",
    ">\n",
    "> 2. $\\mathbf{g_𝑚}=−\\nabla_{𝐹_{𝑚−1} (𝐱)} 𝐿(𝑦, 𝐹_{𝑚−1} (𝐱))$\n",
    ">\n",
    "> 3. $\\mathbf{g_m}$ 을 대상 변수로 $f_m(\\mathbf{x})$ 모델 학습\n",
    ">\n",
    "> 4. $\\gamma_m = \\underset{r}{\\text{argmin}} \\sum_{i=1}^{n} L(y, F_{m-1}(\\mathbf{x}) + \\gamma f_m(\\mathbf{x}))$\n",
    ">\n",
    "> 5. 병합: $F_m(\\mathbf{x})=F_{m-1}(x)+\\text{learning rate} \\times \\gamma_mf_m(\\mathbf{x})$\n",
    ">\n",
    "> 6. m=m + 1, 종료 조건 충족 때까지 2 부터 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba5ce95",
   "metadata": {},
   "source": [
    "- 학습 알고리즘 Square Loss : $L(y, F_m(\\mathbf{x})) = \\frac{1}{2}(y - F_m(\\mathbf{x}))^2$\n",
    "\n",
    "> $g_m = -\\nabla_{F_{m-1}(\\mathbf{x})} L(y, F_{m-1}(\\mathbf{x}))$\n",
    ">\n",
    "> $\\nabla_{F_m(\\mathbf{x})} L(y, F_m(\\mathbf{x})) = -(y - F_m(\\mathbf{x}))$\n",
    "\n",
    "> $\\gamma_m = \\underset{r}{\\text{argmin}} \\sum_{i=1}^{n} L(y_i, F_{m-1}(\\mathbf{x}_i) + \\gamma f_m(\\mathbf{x}_i))$\n",
    ">\n",
    "> $\\gamma_m = \\underset{r}{\\text{argmin}} \\sum_{i=1}^{n} \\frac{1}{2}(y_i - F_{m-1}(\\mathbf{x}_i) - \\gamma f_m(\\mathbf{x}_i))^2$\n",
    "\n",
    "Closed-form의 최적값 도출 가능 : 미분하여 0이 되는 $\\gamma_m$를 찾으면\n",
    "\n",
    "> $\\gamma_m = \\frac{\\sum_{i=1}^{n} f_m(\\mathbf{x}_i)(y_i - F_{m-1}(\\mathbf{x}_i))}{\\sum_{i=1}^{n} f_m(\\mathbf{x}_i)^2}$\n",
    "\n",
    "※불가능 시 : 최적화 알고리즘을 통한 도출(Ex. Linear search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75ffbbb",
   "metadata": {},
   "source": [
    "\n",
    "> 1. $F_0=\\underset{\\boldsymbol{const}}{min} L(y_i, const)$ 손실을 최소화하는 절편 산출, m=1\n",
    ">\n",
    "> 2. $\\mathbf{g_𝑚}=−\\nabla_{𝐹_{𝑚−1} (𝐱)} 𝐿(𝑦, 𝐹_{𝑚−1} (𝐱))$\n",
    ">\n",
    ">    $\\nabla_{F_m(\\mathbf{x})} L(y, F_m(\\mathbf{x})) = -(y - F_m(\\mathbf{x}))$\n",
    ">  \n",
    "> 3. $\\mathbf{g_m}$ 을 대상 변수로 $f_m(\\mathbf{x})$ 모델 학습\n",
    ">\n",
    "> 4. $\\gamma_m = \\underset{r}{\\text{argmin}} \\sum_{i=1}^{n} L(y, F_{m-1}(\\mathbf{x}) + \\gamma f_m(\\mathbf{x}))$\n",
    ">\n",
    ">    $\\gamma_m = \\frac{\\sum_{i=1}^{n} f_m(\\mathbf{x}_i)(y_i - F_{m-1}(\\mathbf{x}_i))}{\\sum_{i=1}^{n} f_m(\\mathbf{x}_i)^2}$\n",
    ">    \n",
    "> 6. 병합: $F_m(\\mathbf{x})=F_{m-1}(x)+\\text{learning rate} \\times \\gamma_mf_m(\\mathbf{x})$\n",
    ">\n",
    "> 7. m=m + 1, 종료 조건 충족 때까지 2 부터 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf83e9e1",
   "metadata": {},
   "source": [
    "### 4. 보팅(Voting)\n",
    "\n",
    "- 여러 종류의 예측기 결합\n",
    "\n",
    "> $𝐹_𝑀(𝐱)=\\sum_{𝑚=1}^𝑀\\gamma_𝑚𝑓_𝑚(𝐱)$\n",
    "> \n",
    "> M: 모델 개수\n",
    "> \n",
    "> $𝑓_1(𝐱),𝑓_2(𝐱), …,𝑓_𝑀(𝐱)$: 기반 모델 $\\Rightarrow$ 여러 종류의 기반 모델을 사용할 수 있음 Ex) f 1(x): 선형 회귀, f 2(x): 경사 부스팅, …\n",
    "> \n",
    "> $\\gamma_1,\\gamma_2, …,\\gamma_𝑀$: 가중치 $\\Rightarrow$ $\\frac{1}{M}$ 또는 개별 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e009fa",
   "metadata": {},
   "source": [
    "#### Hard vs Soft Voting: 분류에서 최종 클래스 선정법\n",
    "\n",
    "- 분류에서 최종 클래스 선정법\n",
    "> Hard Voting(다수결 방식) : 각 모델이 가장 많이 예측한 클래스를 최종 예측으로 결정\n",
    "> \n",
    "> Soft Voting: 각 모델이 예측한 클래스 확률의 (가중)평균을 기반으로 결정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059c7e4f",
   "metadata": {},
   "source": [
    "### 5. 정리\n",
    "\n",
    "#### 특징\n",
    "\n",
    "|   | 결과의 독립성 | 모델의 다양성 | 완전히 틀리지 않음 | 효과적인 결합 방식 |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 배깅<br/>(Bagging) | Boot Strapping | Boot Strapping 을 통한 학습 데이터의 다양성 | 강한 학습기(Strong Learning)를 통해 학습 | 손실을 줄이지 못함 |\n",
    "| AdaBoost | 결합 모델의 손실을 가중치에 반영 | 기반 모델을 학습할 때의 가중치의 변화를 통해 | 손실 큰 데이터 포인트들을 집중 학습 | 손실을 최소화한 가중치 기반의 결합 |\n",
    "| 경사 부스팅<br/>(Gradient Boosting) | 결합 모델의 손실의 경사도를 대상변수 | 손실의 경사도의 변화를 통해 | 이전 단계 결합 모델의 손실을 낮추기 위한 경사도 학습 | 손실을 최소화한 가중치 기반의 결합 |\n",
    "| 보팅<br/>(Voting) | 모델의 다양함에 의함 | 여러 모델을 사용 | 개별 모델의 성능에 따라 다름 | 손실을 줄이지 못함 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ad5dfe",
   "metadata": {},
   "source": [
    "#### 장단점\n",
    "\n",
    "|   | 장점 | 단점 |\n",
    "| --- | --- | --- |\n",
    "| 배깅<br/>(Bagging) | 과적합(Overfitting) 방지에 강함<br/>병렬 처리가 용이하여 빠른 학습<br/>데이터 노이즈에 강함 | 편향(Bias)은 줄이기 어려움<br/>의사결정나무 외에 사용 시 효과가 제한적<br/>너무 단순한 모델은 성능 향상 미미 |\n",
    "| AdaBoost | 편향(Bias) 감소에 효과적<br/>단순하면서 강력함 | 이상치에 민감<br/>순차 학습이라 병렬화가 어려움<br/>오차율 ≥ 0.5 모델은 학습 불가 |\n",
    "| 경사 부스팅<br/>(Gradient Boosting) | 뛰어난 예측 성능<br/>손실 함수 직접 최적화 가능<br/>다양한 손실(목적)함수 지원<br/>Regularization, early stopping 등 다양한 성능 조절 수단 | 과적합 위험 (튜닝 필수)<br/>학습 시간이 오래 걸릴 수 있음<br/>하이퍼 파라미터 튜닝이 복잡함<br/>병렬화가 어려움 |\n",
    "| 보팅<br/>(Voting) | 여러 다른 모델을 쉽게 조합<br/>구현이 간단함, 직관성<br/>성능 안정화 효과<br/>서로 다른 알고리즘 결합 가능 | 잘못된 모델 포함 시 성능 저하<br/>모든 모델 동등 취급(가중치 설정이 어려움)<br/>데이터 분포에 따라 성능 편차가 클 수 있음 |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
